{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we aim to build a predictive model that receives an input array of city statistics and outputs a predicted metric for “nonViolPerPop” which represents the total number of non-violent crimes per 100K in the city’s population. We approached this problem by first reading in and cleaning the data and then building three predictive models, a Random Forest Regressor, a Lasso regression analysis, and a Decision Tree Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import shutup; shutup.please()\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import arff\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>pop</th>\n",
       "      <th>perHoush</th>\n",
       "      <th>pctBlack</th>\n",
       "      <th>pctWhite</th>\n",
       "      <th>pctAsian</th>\n",
       "      <th>pctHisp</th>\n",
       "      <th>pct12-21</th>\n",
       "      <th>pct12-29</th>\n",
       "      <th>pct16-24</th>\n",
       "      <th>...</th>\n",
       "      <th>burglaries</th>\n",
       "      <th>burglPerPop</th>\n",
       "      <th>larcenies</th>\n",
       "      <th>larcPerPop</th>\n",
       "      <th>autoTheft</th>\n",
       "      <th>autoTheftPerPop</th>\n",
       "      <th>arsons</th>\n",
       "      <th>arsonsPerPop</th>\n",
       "      <th>violentPerPop</th>\n",
       "      <th>nonViolPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NJ</td>\n",
       "      <td>11980.0</td>\n",
       "      <td>3.10</td>\n",
       "      <td>1.37</td>\n",
       "      <td>91.78</td>\n",
       "      <td>6.50</td>\n",
       "      <td>1.88</td>\n",
       "      <td>12.47</td>\n",
       "      <td>21.44</td>\n",
       "      <td>10.93</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>114.85</td>\n",
       "      <td>138.0</td>\n",
       "      <td>1132.08</td>\n",
       "      <td>16.0</td>\n",
       "      <td>131.26</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.41</td>\n",
       "      <td>41.02</td>\n",
       "      <td>1394.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PA</td>\n",
       "      <td>23123.0</td>\n",
       "      <td>2.82</td>\n",
       "      <td>0.80</td>\n",
       "      <td>95.57</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.85</td>\n",
       "      <td>11.01</td>\n",
       "      <td>21.30</td>\n",
       "      <td>10.48</td>\n",
       "      <td>...</td>\n",
       "      <td>57.0</td>\n",
       "      <td>242.37</td>\n",
       "      <td>376.0</td>\n",
       "      <td>1598.78</td>\n",
       "      <td>26.0</td>\n",
       "      <td>110.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.25</td>\n",
       "      <td>127.56</td>\n",
       "      <td>1955.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OR</td>\n",
       "      <td>29344.0</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.74</td>\n",
       "      <td>94.33</td>\n",
       "      <td>3.43</td>\n",
       "      <td>2.35</td>\n",
       "      <td>11.36</td>\n",
       "      <td>25.88</td>\n",
       "      <td>11.01</td>\n",
       "      <td>...</td>\n",
       "      <td>274.0</td>\n",
       "      <td>758.14</td>\n",
       "      <td>1797.0</td>\n",
       "      <td>4972.19</td>\n",
       "      <td>136.0</td>\n",
       "      <td>376.30</td>\n",
       "      <td>22.0</td>\n",
       "      <td>60.87</td>\n",
       "      <td>218.59</td>\n",
       "      <td>6167.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NY</td>\n",
       "      <td>16656.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.70</td>\n",
       "      <td>97.35</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.70</td>\n",
       "      <td>12.55</td>\n",
       "      <td>25.20</td>\n",
       "      <td>12.19</td>\n",
       "      <td>...</td>\n",
       "      <td>225.0</td>\n",
       "      <td>1301.78</td>\n",
       "      <td>716.0</td>\n",
       "      <td>4142.56</td>\n",
       "      <td>47.0</td>\n",
       "      <td>271.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>306.64</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MN</td>\n",
       "      <td>11245.0</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.53</td>\n",
       "      <td>89.16</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.52</td>\n",
       "      <td>24.46</td>\n",
       "      <td>40.53</td>\n",
       "      <td>28.69</td>\n",
       "      <td>...</td>\n",
       "      <td>91.0</td>\n",
       "      <td>728.93</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>8490.87</td>\n",
       "      <td>91.0</td>\n",
       "      <td>728.93</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9988.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  State      pop  perHoush  pctBlack  pctWhite  pctAsian  pctHisp  pct12-21  \\\n",
       "0    NJ  11980.0      3.10      1.37     91.78      6.50     1.88     12.47   \n",
       "1    PA  23123.0      2.82      0.80     95.57      3.44     0.85     11.01   \n",
       "2    OR  29344.0      2.43      0.74     94.33      3.43     2.35     11.36   \n",
       "3    NY  16656.0      2.40      1.70     97.35      0.50     0.70     12.55   \n",
       "4    MN  11245.0      2.76      0.53     89.16      1.17     0.52     24.46   \n",
       "\n",
       "   pct12-29  pct16-24  ...  burglaries  burglPerPop  larcenies  larcPerPop  \\\n",
       "0     21.44     10.93  ...        14.0       114.85      138.0     1132.08   \n",
       "1     21.30     10.48  ...        57.0       242.37      376.0     1598.78   \n",
       "2     25.88     11.01  ...       274.0       758.14     1797.0     4972.19   \n",
       "3     25.20     12.19  ...       225.0      1301.78      716.0     4142.56   \n",
       "4     40.53     28.69  ...        91.0       728.93     1060.0     8490.87   \n",
       "\n",
       "   autoTheft  autoTheftPerPop  arsons  arsonsPerPop  violentPerPop  \\\n",
       "0       16.0           131.26     2.0         16.41          41.02   \n",
       "1       26.0           110.55     1.0          4.25         127.56   \n",
       "2      136.0           376.30    22.0         60.87         218.59   \n",
       "3       47.0           271.93     NaN           NaN         306.64   \n",
       "4       91.0           728.93     5.0         40.05            NaN   \n",
       "\n",
       "   nonViolPerPop  \n",
       "0        1394.59  \n",
       "1        1955.95  \n",
       "2        6167.51  \n",
       "3            NaN  \n",
       "4        9988.79  \n",
       "\n",
       "[5 rows x 143 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "\n",
    "df = pd.read_csv('data/crime/CommViolPredUnnormalizedData.txt', encoding='latin-1',header=None)\n",
    "df.columns\n",
    "#the column names are the second word in each row of data/crime_headings.txt\n",
    "with open('data/crime/crime_headings.txt') as f:\n",
    "    headings = f.readlines()\n",
    "col_names = []\n",
    "types = []\n",
    "for heading in headings:\n",
    "    if len(heading.split()) <= 1:\n",
    "        continue\n",
    "    col_names.append(heading.split()[1])\n",
    "    if heading.split()[2] == 'numeric':\n",
    "        types.append(float)\n",
    "    else:\n",
    "        types.append(str)\n",
    "\n",
    "df.columns = col_names\n",
    "\n",
    "#drop drop rows with \"?\" values\n",
    "df = df.replace('?', np.nan)\n",
    "\n",
    "df = df.astype(dict(zip(col_names, types)))\n",
    "\n",
    "#communityname, countyCode, communityCode, fold are not predictive so drop them\n",
    "df = df.drop(['communityname', 'countyCode', 'communityCode', 'fold'], axis=1)\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the data using pandas read_csv in Python. The original data set did not include column names, so we read in the column names from a separate file and inserted those into our data frame. We did some data preprocessing and formatting of columns and dropped columns that are not predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful functions\n",
    "def drop_rows_missing_target(df):\n",
    "    return df.dropna(subset=['nonViolPerPop'])\n",
    "\n",
    "\n",
    "def fill_with_mean(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == float:\n",
    "            df[column] = df[column].fillna(df[column].mean())\n",
    "    return df\n",
    "\n",
    "def fill_with_median(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == float:\n",
    "            df[column] = df[column].fillna(df[column].median())\n",
    "    return df\n",
    "\n",
    "def convert_categorical_to_numeric(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype != float:\n",
    "            df[column] = df[column].astype('category')\n",
    "            df[column] = df[column].cat.codes\n",
    "    return df\n",
    "\n",
    "def normalize(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == float:\n",
    "            df[column] = (df[column] - df[column].mean()) / df[column].std()\n",
    "    return df\n",
    "\n",
    "def remove_random_features(df, n):\n",
    "    dropped_cols = np.random.choice(df.columns[:-1], n, replace=False)\n",
    "    return df.drop(dropped_cols, axis=1), dropped_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to remove the missing values of the dataset as effectively as possible. We first dropped the columns with more that 80% missing values. Then after dropping the columns with more than 80% \n",
    "missing values, we then dropped all rows that also have a missing value in our target nonViolPerPop. After that we dropped all the remaining columns with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2118, 121)\n",
      "(2118, 121)\n",
      "(2118, 121)\n",
      "(2118, 121)\n"
     ]
    }
   ],
   "source": [
    "# function to drop the columnns with more that 90% missing values\n",
    "print(df.shape)\n",
    "def drop_missing_values(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].isnull().sum() > 0.9*len(df):\n",
    "            df = df.drop(column, axis=1)\n",
    "    return df\n",
    "# drop the columns with more than 80% missing values\n",
    "df = drop_missing_values(df)\n",
    "print(df.shape)\n",
    "# drop the rows with missing values from the nonViolPerPop column\n",
    "df = drop_rows_missing_target(df)\n",
    "print(df.shape)\n",
    "#drop the remaining columns with missing values\n",
    "df = df.dropna(axis=1)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Random Forest Regressor (RFR) is a predictive model that, according to sklearn’s website, “fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.” This means that RFRs take an ensemble approach, using many different random decision trees and aggregating them to create a tree that is greater than the sum of their parts. \n",
    "\n",
    "Random Forest Regressors have an optional fine-tuning approach, in which the model is trained with the entire dataset and the model gains an attribute “feature_importances_”. Using these feature importances we were able to highlight features that were most important in predicting output and drop other features from our model. \n",
    "\n",
    "With our RFR we saw impressive results with both fine tuning the features and without. The results can be seen below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using targets as features:\n",
      "Mean Absolute Error: 0.06659212833266745\n",
      "Kept columns: ['larcPerPop', 'burglPerPop', 'autoTheftPerPop', 'nonViolPerPop']\n",
      "\n",
      "\n",
      "\n",
      "Not using targets as features:\n",
      "Mean Absolute Error: 0.49681147064804626\n",
      "Kept columns: ['pctKids2Par', 'pct2Par', 'pctAllDivorc', 'pct12-17w2Par', 'rentLowQ', 'pctPopDenseHous', 'pctMaleDivorc', 'persHomeless', 'pctHousOccup', 'houseVacant', 'pctFemDivorc', 'persPerRenterOccup', 'persPoverty', 'kidsBornNevrMarr', 'nonViolPerPop']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def feature_selection(df, features_as_targets):\n",
    "    #drop the last 18 columns\n",
    "    if features_as_targets:\n",
    "        X = df.drop(df.columns[-1:], axis=1)\n",
    "    else:\n",
    "        X = df.drop(df.columns[-18:], axis=1)\n",
    "        \n",
    "    y = df['nonViolPerPop']\n",
    "    rf = RandomForestRegressor(n_estimators=100)\n",
    "    rf.fit(X, y)\n",
    "    importance = rf.feature_importances_\n",
    "    # print(np.sort(importance))\n",
    "    indices = np.argsort(importance)[::-1]\n",
    "    kept_cols = []\n",
    "    for f in range(X.shape[1]):\n",
    "        if importance[indices[f]] > 0.01:\n",
    "            kept_cols.append(X.columns[indices[f]])\n",
    "    #put nonViolPerPop back in\n",
    "    kept_cols.append('nonViolPerPop')\n",
    "    return df[kept_cols], kept_cols\n",
    "\n",
    "\n",
    "def fine_tune_features_approach(df, features_as_targets = False):\n",
    "    black_box_df = fill_with_mean(normalize(drop_rows_missing_target(convert_categorical_to_numeric(df))))\n",
    "\n",
    "    black_box_df, kept_cols = feature_selection(black_box_df, features_as_targets)\n",
    "\n",
    "    train, test = train_test_split(black_box_df, test_size=0.2)\n",
    "\n",
    "    X_train = train.drop(train.columns[-1], axis=1)\n",
    "    y_train = train['nonViolPerPop']\n",
    "    X_test = test.drop(test.columns[-1], axis=1)\n",
    "    y_test = test['nonViolPerPop']\n",
    "\n",
    "    # Create the model with 100 trees\n",
    "    model = RandomForestRegressor(n_estimators=100,\n",
    "                                    bootstrap = True,\n",
    "                                    max_features = 'sqrt')\n",
    "    # Fit on training data\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Actual class predictions\n",
    "    rf_predictions = model.predict(X_test)\n",
    "\n",
    "    #calculate mae\n",
    "    mae = np.mean(abs(rf_predictions - y_test))\n",
    "    print('Mean Absolute Error:', mae)\n",
    "    print('Kept columns:', kept_cols)\n",
    "    print()\n",
    "print('Using targets as features:')\n",
    "fine_tune_features_approach(df, True)\n",
    "print('\\n\\nNot using targets as features:')\n",
    "fine_tune_features_approach(df, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using targets as features (normalize):\n",
      "Mean Absolute Error: 0.23966561863969835\n",
      "\n",
      "\n",
      "Not using targets as features (normalize):\n",
      "Mean Absolute Error: 0.48363125581579414\n",
      "\n",
      "\n",
      "Using targets as features (no normalize):\n",
      "Mean Absolute Error: 514.6430143867925\n",
      "\n",
      "\n",
      "Not using targets as features (no normalize):\n",
      "Mean Absolute Error: 1267.8243113207548\n"
     ]
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/random-forest-regression-in-python/\n",
    "\n",
    "def random_forest_approach(df, features_as_targets = False, do_normalize = False):\n",
    "    for i in range(1):\n",
    "        black_box_df = df.copy()\n",
    "\n",
    "        if not features_as_targets:\n",
    "            black_box_df = black_box_df.drop(black_box_df.columns[-17:-1], axis=1)\n",
    "        if do_normalize:\n",
    "            black_box_df = fill_with_mean(normalize(drop_rows_missing_target(convert_categorical_to_numeric(black_box_df))))\n",
    "        else:\n",
    "            black_box_df = fill_with_mean(drop_rows_missing_target(convert_categorical_to_numeric(black_box_df)))\n",
    "\n",
    "\n",
    "        train, test = train_test_split(black_box_df, test_size=0.2)\n",
    "        X_train = train.drop(train.columns[-1], axis=1)\n",
    "        y_train = train['nonViolPerPop']\n",
    "        X_test = test.drop(test.columns[-1], axis=1)\n",
    "        y_test = test['nonViolPerPop']\n",
    "\n",
    "        # Create the model with 100 trees\n",
    "        model = RandomForestRegressor(n_estimators=100,\n",
    "                                        bootstrap = True,\n",
    "                                        max_features = 'sqrt')\n",
    "        # Fit on training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Actual class predictions\n",
    "        rf_predictions = model.predict(X_test)\n",
    "\n",
    "        #calculate mae\n",
    "        mae = np.mean(abs(rf_predictions - y_test))\n",
    "        print('Mean Absolute Error:', mae)\n",
    "\n",
    "        \n",
    "        \n",
    "print('Using targets as features (normalize):')\n",
    "random_forest_approach(df, True, True)\n",
    "print('\\n\\nNot using targets as features (normalize):')\n",
    "random_forest_approach(df, False, True)\n",
    "print('\\n\\nUsing targets as features (no normalize):')\n",
    "random_forest_approach(df, True, False)\n",
    "print('\\n\\nNot using targets as features (no normalize):')\n",
    "random_forest_approach(df, False, False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Lasso regression model is a regression that penalizes factors in order to “select” the best ones for the model. Using the parameter lambda to identify the penalizing term, it forces factors that have little to no effect on the response variable to have a coefficient term of zero. Factors that have non-zero coefficients are “selected” and have an effect on the model.\n",
    "We fit four different Lasso models using different strategies. Two models were fitted when we included other response variables in the data set as factors, and two other models were fitted excluding response variables as factors. One of the models from each of those two pairs used normalized data, while the other model from each was not normalized.\n",
    "We used the LassoCV package in Python to find the best value of the parameter for the penalization term and then fit our models. Each model had differing results. The models that included other response variables as factors performed the best, with low mean absolute error and high r-squared values. When we normalized our data, the performance of our models improved substantially. Those models have leakage, however, which skews our results. Not using targets as features, our results were more realistic but not as good. Our r-squared values were much lower with these models and our mean absolute error was much higher. All significant factors included in the models are shown below, but some of our selected variables are whitePerCap, medFamIncome, and persPerEccupHous. It was interesting to see the different variables the models selected and the different coefficients they had.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using targets as features (normalized):\n",
      "Mean Absolute Error: 0.0010973321789923415\n",
      "kept Columns: ['larcPerPop: 0.69865', 'burglPerPop: 0.27935', 'autoTheftPerPop: 0.18385', 'arsonsPerPop: 0.01365', 'State: -0.00001']\n",
      "R squared: 0.9999975761617778\n",
      "\n",
      "\n",
      "Not using targets as features (normalized):\n",
      "Mean Absolute Error: 0.46124457718071876\n",
      "kept Columns: ['persPerOccupHous: 0.32557', 'pctWsocsec: 0.32228', 'pctForeignBorn: 0.29128', 'pctMaleDivorc: 0.25351', 'pctEmploy: 0.25221', 'pctImmig-10: -0.24941', 'pctMaleNevMar: 0.23886', 'whitePerCap: 0.22923', 'pctPoverty: 0.21450', 'pctKids2Par: -0.15475', 'persPerOwnOccup: -0.13792', 'rentLowQ: -0.13377', 'perHoush: -0.13326', 'pctLowEdu: -0.12557', 'medYrHousBuilt: 0.12169', 'pctOccupMgmt: 0.11420', 'pctKidsBornNevrMarr: 0.10331', 'pctFgnImmig-5: -0.09419', 'pctFgnImmig-8: 0.09031', 'ownHousUperQ: -0.08896', 'pctRetire: -0.08769', 'pctSmallHousUnits: -0.08751', 'pctCollGrad: -0.08698', 'medOwnCostpct: -0.08124', 'kidsBornNevrMarr: -0.08069', 'perCapInc: -0.07583', 'pctSpeakOnlyEng: -0.07508', 'pctFemDivorc: 0.07286', 'pctHousOccup: -0.07157', 'pctFgnImmig-10: 0.06650', 'medNumBedrm: -0.06636', 'rentUpperQ: -0.06233', 'pct16-24: -0.05199', 'persPerRenterOccup: -0.05186', 'pctEmployMfg: -0.04814', 'persEmergShelt: 0.04694', 'pctBornStateResid: -0.04558', 'pctImmig-3: -0.04558', 'pct65up: 0.04450', 'pctLargHousFam: 0.04345', 'pctAsian: 0.04242', 'pctVacant6up: -0.04226', 'pctBlack: 0.03868', 'pctNotHSgrad: -0.03714', 'pctNotSpeakEng: 0.03648', 'pctEmployProfServ: -0.03486', 'pctWorkMom-18: -0.02839', 'pct12-17w2Par: -0.02556', 'medOwnCostPctWO: -0.02373', 'pctUrban: 0.02253', 'pctPopDenseHous: 0.02239', 'asianPerCap: -0.01872', 'pctHisp: -0.01753', 'blackPerCap: -0.01457', 'pctWorkMom-6: -0.01455', 'ownHousLowQ: -0.01398', 'pctPersOwnOccup: -0.01220', 'pctHousWOplumb: -0.01125', 'numForeignBorn: 0.00918', 'pctWdiv: 0.00872', 'pctVacantBoarded: -0.00473', 'pctWfarm: -0.00460', 'houseVacant: 0.00412', 'pctPubAsst: -0.00235', 'persHomeless: 0.00228', 'medRentpctHousInc: 0.00202', 'pctHousWOphone: 0.00134', 'hispPerCap: 0.00044', 'State: 0.00028']\n",
      "R squared: 0.5252698070219908\n",
      "\n",
      "\n",
      "Using targets as features (not normalized):\n",
      "Mean Absolute Error: 113.03345839349367\n",
      "kept Columns: ['larcPerPop: 1.02423', 'burglPerPop: 0.93458', 'autoTheftPerPop: 0.65272', 'medFamIncome: -0.00822', 'popDensity: 0.00708', 'persUrban: 0.00197', 'numForeignBorn: -0.00184', 'pop: -0.00145', 'persPoverty: 0.00075', 'ownHousMed: 0.00067', 'ownHousUperQ: 0.00040', 'NAperCap: 0.00016']\n",
      "R squared: 0.9949530734857374\n",
      "\n",
      "\n",
      "Not using targets as features (not normalized):\n",
      "Mean Absolute Error: 1561.5632151912807\n",
      "kept Columns: ['whitePerCap: 0.57715', 'perCapInc: -0.30896', 'medFamIncome: -0.17386', 'kidsBornNevrMarr: -0.04080', 'persUrban: 0.01480', 'persPoverty: 0.01348', 'medIncome: -0.01272', 'pop: -0.01224', 'numForeignBorn: -0.00880', 'ownHousMed: 0.00852', 'asianPerCap: -0.00807', 'ownHousLowQ: -0.00585', 'blackPerCap: -0.00459', 'ownHousUperQ: -0.00310', 'NAperCap: 0.00138']\n",
      "R squared: 0.4234363479573525\n"
     ]
    }
   ],
   "source": [
    "#doing lasso \n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "def lasso(df, features_as_targets, do_normalize):\n",
    "    if do_normalize:\n",
    "        lasso_df = fill_with_mean(normalize(drop_rows_missing_target(convert_categorical_to_numeric(df))))\n",
    "    else:\n",
    "        lasso_df = fill_with_mean(drop_rows_missing_target(convert_categorical_to_numeric(df)))\n",
    "\n",
    "\n",
    "\n",
    "    if features_as_targets:\n",
    "        X = lasso_df.drop(lasso_df.columns[-1:], axis=1)\n",
    "    else:\n",
    "        X = lasso_df.drop(lasso_df.columns[-18:], axis=1)\n",
    "    y = lasso_df['nonViolPerPop']\n",
    "\n",
    "    alpha_predict = LassoCV(cv=5, random_state=0, max_iter=10000)\n",
    "    alpha_predict.fit(X, y)\n",
    "\n",
    "    model = Lasso(alpha=alpha_predict.alpha_)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = np.mean(abs(y_pred - y_test))\n",
    "    print('Mean Absolute Error:', mae)\n",
    "    coef_dict = { k:v for (k,v) in zip(model.coef_, X)}\n",
    "    #sort coef_dict by abs value\n",
    "    coef_dict = {k: v for k, v in sorted(coef_dict.items(), key=lambda item: abs(item[0]), reverse=True)}\n",
    "\n",
    "    print('kept Columns:', [f'{v}: {k:.5f}' for k,v in coef_dict.items() if k > 0.00001 or k < -0.00001])\n",
    "    #print R squared\n",
    "    print('R squared:', model.score(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Using targets as features (normalized):')\n",
    "lasso(df, True, True)\n",
    "print('\\n\\nNot using targets as features (normalized):')\n",
    "lasso(df, False, True)\n",
    "print('\\n\\nUsing targets as features (not normalized):')\n",
    "lasso(df, True, False)\n",
    "print('\\n\\nNot using targets as features (not normalized):')\n",
    "lasso(df, False, False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Decision Tree is a non-parametric supervised learning method. Sklearn says a decision tree “predicts the value of a target variable by learning simple decision rules inferred from the data features”. We used the default tree which uses a gini impurity function to determine the best way to split. When training the gini function looks at the relationship between all of the features and determines the best split. This continues until a full decision tree is made. Then when we are running our predictions our data is run through the data and classified by the tree. \n",
    "\n",
    "With our Decision Tree we saw some impressive results when we used the targets as features but we saw not as good of results when we didn’t. The results can be seen below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using targets as features (normalized):\n",
      "Mean Absolute Error: 0.1094720955465372\n",
      "\n",
      "\n",
      "Not using targets as features (normalized):\n",
      "Mean Absolute Error: 0.6719314177846643\n",
      "\n",
      "\n",
      "Using targets as features (not normalized):\n",
      "Mean Absolute Error: 324.0136320754718\n",
      "\n",
      "\n",
      "Not using targets as features (not normalized):\n",
      "Mean Absolute Error: 1918.6865566037736\n"
     ]
    }
   ],
   "source": [
    "def dec_tree(df, features_as_targets, do_normalize):\n",
    "    if do_normalize:\n",
    "        dec_df = fill_with_mean(normalize(drop_rows_missing_target(convert_categorical_to_numeric(df))))\n",
    "    else:\n",
    "        dec_df = fill_with_mean(drop_rows_missing_target(convert_categorical_to_numeric(df)))\n",
    "\n",
    "    if features_as_targets:\n",
    "        X = dec_df.drop(dec_df.columns[-1:], axis=1)\n",
    "    else:\n",
    "        X = dec_df.drop(dec_df.columns[-18:], axis=1)\n",
    "    y = dec_df['nonViolPerPop']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    dtree = tree.DecisionTreeRegressor()\n",
    "    dtree = dtree.fit(X_train, y_train)\n",
    "    preds = dtree.predict(X_test)\n",
    "\n",
    "    mae = np.mean(abs(preds - y_test))\n",
    "    print('Mean Absolute Error:', mae)\n",
    "\n",
    "print('Using targets as features (normalized):')\n",
    "dec_tree(df, True, True)\n",
    "print('\\n\\nNot using targets as features (normalized):')\n",
    "dec_tree(df, False, True)\n",
    "print('\\n\\nUsing targets as features (not normalized):')\n",
    "dec_tree(df, True, False)\n",
    "print('\\n\\nNot using targets as features (not normalized):')\n",
    "dec_tree(df, False, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise was very helpful in understanding how to create explainable models, do feature engineering, and make predictions on a large dataset. In our experiments we found that our best performing model (by far) was the Random Forest Regressor (when not using other targets as features) with a mean absolute error of .484. \n",
    "\n",
    "Given more time, we think it would be interesting to implement a more black box model and see how its prediction performance compares to those of our models. This would help us further understand how much performance improvement can be gained at the expense of explainability. \n",
    "\n",
    "In addition, we would be interested in trying out different hyperparameters on our models, perhaps with a grid search, so we could truly optimize for both performance AND explainability.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
